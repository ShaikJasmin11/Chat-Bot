{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6cILYT0nCiK",
        "outputId": "88d76b71-a844-47c2-a38f-3299b6217de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy pandas nltk transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "d5dp7L1isJ6a",
        "outputId": "8983bf4c-3a55-4625-f1f6-1e80c212fa84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-615c69b2-1b49-45c1-a27c-8b171b2dbb2b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-615c69b2-1b49-45c1-a27c-8b171b2dbb2b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving .DS_Store to .DS_Store\n",
            "Saving chameleons.pdf to chameleons.pdf\n",
            "Saving movie_characters_metadata.txt to movie_characters_metadata.txt\n",
            "Saving movie_conversations.txt to movie_conversations.txt\n",
            "Saving movie_lines.txt to movie_lines.txt\n",
            "Saving movie_titles_metadata.txt to movie_titles_metadata.txt\n",
            "Saving raw_script_urls.txt to raw_script_urls.txt\n",
            "Saving README.txt to README.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the movie lines\n",
        "lines_file = \"movie_lines.txt\"\n",
        "# Specify the encoding as 'latin-1'\n",
        "lines = pd.read_csv(lines_file, sep=\" \\+\\+\\+\\$\\+\\+\\+ \", header=None, engine=\"python\", names=[\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"], encoding='latin-1')\n",
        "# If 'latin-1' doesn't work, try 'cp1252'\n",
        "# lines = pd.read_csv(lines_file, sep=\" \\+\\+\\+\\$\\+\\+\\+ \", header=None, engine=\"python\", names=[\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"], encoding='cp1252')\n",
        "\n",
        "# Load the movie conversations\n",
        "dialogue_file = \"movie_conversations.txt\"\n",
        "conversations = pd.read_csv(dialogue_file, sep=\" \\+\\+\\+\\$\\+\\+\\+ \", header=None, engine=\"python\", names=[\"characterID1\", \"characterID2\", \"movieID\", \"utteranceIDs\"], encoding='latin-1') # Also apply encoding to this line\n",
        "# If 'latin-1' doesn't work, try 'cp1252'\n",
        "# conversations = pd.read_csv(dialogue_file, sep=\" \\+\\+\\+\\$\\+\\+\\+ \", header=None, engine=\"python\", names=[\"characterID1\", \"characterID2\", \"movieID\", \"utteranceIDs\"], encoding='cp1252') # Also apply encoding to this line\n",
        "\n",
        "# Convert the utteranceIDs to a list of IDs (as they are stored as strings)\n",
        "conversations[\"utteranceIDs\"] = conversations[\"utteranceIDs\"].apply(lambda x: re.findall(r'L[0-9]+', x))"
      ],
      "metadata": {
        "id": "xnmatsMfxOhg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first few rows of each file to verify loading\n",
        "print(\"Movie Lines Data:\")\n",
        "print(lines.head())\n",
        "\n",
        "print(\"\\nMovie Conversations Data:\")\n",
        "print(conversations.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xuJyjwcxTjU",
        "outputId": "d8c3b979-695d-4c67-a7a3-03aca000d8c6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Movie Lines Data:\n",
            "  lineID characterID movieID character          text\n",
            "0  L1045          u0      m0    BIANCA  They do not!\n",
            "1  L1044          u2      m0   CAMERON   They do to!\n",
            "2   L985          u0      m0    BIANCA    I hope so.\n",
            "3   L984          u2      m0   CAMERON     She okay?\n",
            "4   L925          u0      m0    BIANCA     Let's go.\n",
            "\n",
            "Movie Conversations Data:\n",
            "  characterID1 characterID2 movieID              utteranceIDs\n",
            "0           u0           u2      m0  [L194, L195, L196, L197]\n",
            "1           u0           u2      m0              [L198, L199]\n",
            "2           u0           u2      m0  [L200, L201, L202, L203]\n",
            "3           u0           u2      m0        [L204, L205, L206]\n",
            "4           u0           u2      m0              [L207, L208]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "# Download NLTK data (only required once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load your dataset\n",
        "dialogue_file = \"/content/movie_conversations.txt\"\n",
        "lines_file = \"/content/movie_lines.txt\"\n",
        "\n",
        "# Specify the encoding as 'latin-1' when reading the files\n",
        "conversations = pd.read_csv(dialogue_file, sep=\" \\+\\+\\+\\$\\+\\+\\+ \", header=None, engine='python', encoding='latin-1')\n",
        "lines = pd.read_csv(lines_file, sep=\" \\+\\+\\+\\$\\+\\+\\+ \", header=None, engine='python', encoding='latin-1')  # Apply encoding here\n",
        "\n",
        "# Display the first few rows to identify the correct dialogue column\n",
        "print(lines.head())\n",
        "\n",
        "# Assuming the dialogue text is in column 4 (adjust if necessary)\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower(), flags=re.ASCII)\n",
        "    return text\n",
        "\n",
        "# Handle missing values and apply preprocessing\n",
        "lines[4] = lines[4].fillna('').apply(preprocess)\n",
        "\n",
        "# Preview the preprocessed data\n",
        "print(lines[4].head())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCAU1Sjoyy2j",
        "outputId": "af847dd6-9c60-4c09-9230-e26eab2b6b89"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       0   1   2        3             4\n",
            "0  L1045  u0  m0   BIANCA  They do not!\n",
            "1  L1044  u2  m0  CAMERON   They do to!\n",
            "2   L985  u0  m0   BIANCA    I hope so.\n",
            "3   L984  u2  m0  CAMERON     She okay?\n",
            "4   L925  u0  m0   BIANCA     Let's go.\n",
            "0    they do not\n",
            "1     they do to\n",
            "2      i hope so\n",
            "3       she okay\n",
            "4        lets go\n",
            "Name: 4, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming the data was loaded as described earlier\n",
        "# Verify the structure of the data\n",
        "print(conversations.head())\n",
        "print(lines.head())\n",
        "\n",
        "# Ensure 'lines' DataFrame contains the dialogue column at the expected index\n",
        "dialogue_column_index = 4  # Adjust if necessary, based on your dataset's structure\n",
        "\n",
        "# Create a dictionary to map line IDs to dialogue text for faster lookup\n",
        "lines_dict = {}\n",
        "for _, row in lines.iterrows():\n",
        "    line_id = row[0]  # Assuming line ID is in the first column\n",
        "    dialogue = row[dialogue_column_index]  # Adjust based on the column with dialogue text\n",
        "    lines_dict[line_id] = dialogue\n",
        "\n",
        "# Convert conversations from string to list format\n",
        "try:\n",
        "    conversations[3] = conversations[3].apply(eval)\n",
        "except Exception as e:\n",
        "    print(f\"Error in eval conversion: {e}\")\n",
        "\n",
        "# Create pairs of input-response dialogues\n",
        "pairs = []\n",
        "for conv in conversations[3]:  # Assuming conversation IDs are in the fourth column\n",
        "    for i in range(len(conv) - 1):\n",
        "        input_line = lines_dict.get(conv[i])\n",
        "        response_line = lines_dict.get(conv[i + 1])\n",
        "        if input_line and response_line:  # Only add pairs if both lines exist\n",
        "            pairs.append((input_line, response_line))\n",
        "\n",
        "# Convert to DataFrame\n",
        "conversation_df = pd.DataFrame(pairs, columns=['Input', 'Response'])\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(conversation_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJu6Uj_5zjmV",
        "outputId": "69107ae4-d7b2-4701-c1f4-5003d21d53c9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0   1   2                                 3\n",
            "0  u0  u2  m0  ['L194', 'L195', 'L196', 'L197']\n",
            "1  u0  u2  m0                  ['L198', 'L199']\n",
            "2  u0  u2  m0  ['L200', 'L201', 'L202', 'L203']\n",
            "3  u0  u2  m0          ['L204', 'L205', 'L206']\n",
            "4  u0  u2  m0                  ['L207', 'L208']\n",
            "       0   1   2        3            4\n",
            "0  L1045  u0  m0   BIANCA  they do not\n",
            "1  L1044  u2  m0  CAMERON   they do to\n",
            "2   L985  u0  m0   BIANCA    i hope so\n",
            "3   L984  u2  m0  CAMERON     she okay\n",
            "4   L925  u0  m0   BIANCA      lets go\n",
            "                                               Input  \\\n",
            "0  can we make this quick  roxanne korrine and an...   \n",
            "1  well i thought wed start with pronunciation if...   \n",
            "2  not the hacking and gagging and spitting part ...   \n",
            "3  youre asking me out  thats so cute whats your ...   \n",
            "4  no no its my fault  we didnt have a proper int...   \n",
            "\n",
            "                                            Response  \n",
            "0  well i thought wed start with pronunciation if...  \n",
            "1  not the hacking and gagging and spitting part ...  \n",
            "2  okay then how bout we try out some french cuis...  \n",
            "3                                          forget it  \n",
            "4                                            cameron  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "from transformers import pipeline\n",
        "\n",
        "# Step 2: Load the zero-shot classification pipeline\n",
        "# (This may download a model for the first time, so ensure you have an internet connection)\n",
        "nlu_pipeline = pipeline('zero-shot-classification')\n",
        "\n",
        "# Step 3: Define possible intents\n",
        "intents = [\"greeting\", \"goodbye\", \"thanks\", \"no answer\"]\n",
        "\n",
        "# Step 4: Create a function to classify intent\n",
        "def classify_intent(input_text):\n",
        "    result = nlu_pipeline(input_text, candidate_labels=intents)\n",
        "    return result['labels'][0]  # Get the top label (intent)\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"Hello, how are you?\"\n",
        "intent = classify_intent(input_text)\n",
        "print(f\"Detected intent: {intent}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toAg_u0J0LsC",
        "outputId": "c89db6e1-b0e8-4ff8-eabf-63ba94f4bd1a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected intent: greeting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load the GPT-2 model and tokenizer\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, cache_dir='/content/cache')\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name, cache_dir='/content/cache')\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def generate_response(input_text):\n",
        "    try:\n",
        "        # Truncate long inputs\n",
        "        max_input_length = 512\n",
        "        tokens = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "        if tokens.shape[-1] > max_input_length:\n",
        "            tokens = tokens[:, -max_input_length:]\n",
        "\n",
        "        # Generate response\n",
        "        outputs = model.generate(tokens, max_length=100, num_return_sequences=1)\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(\"Error during response generation:\", e)\n",
        "        return \"I'm sorry, I couldn't process your request.\"\n",
        "\n",
        "# Test the function\n",
        "print(generate_response(\"Hello, how are you?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KRrHrXgEcje",
        "outputId": "f5a89afa-dd0f-4c97-8f10-b5154a8943f6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, how are you?\n",
            "\n",
            "I'm a little bit of a nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Flask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI7IM8EmE4zN",
        "outputId": "c2b0e873-43ae-4d46-a218-020429596da7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, GPT2LMHeadModel, GPT2Tokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Step 1: Intent Classification (Using DistilBERT for simplicity)\n",
        "\n",
        "# Load the DistilBERT model and tokenizer\n",
        "intent_model_name = 'distilbert-base-uncased'\n",
        "intent_tokenizer = BertTokenizer.from_pretrained(intent_model_name)\n",
        "intent_model = BertForSequenceClassification.from_pretrained(intent_model_name, num_labels=3)  # Modify for your intent categories\n",
        "\n",
        "# Intent labels\n",
        "intent_labels = [\"greeting\", \"weather\", \"goodbye\"]\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(intent_labels)\n",
        "\n",
        "# Intent Classification Function\n",
        "def classify_intent(user_input):\n",
        "    inputs = intent_tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
        "    with torch.no_grad():\n",
        "        outputs = intent_model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_class_id = torch.argmax(logits, dim=1).item()\n",
        "    intent = label_encoder.inverse_transform([predicted_class_id])[0]\n",
        "    return intent\n",
        "\n",
        "# Step 2: Response Generation (Using GPT-2)\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "response_model_name = 'gpt2'\n",
        "response_tokenizer = GPT2Tokenizer.from_pretrained(response_model_name)\n",
        "response_model = GPT2LMHeadModel.from_pretrained(response_model_name)\n",
        "\n",
        "# Response Generation Function using GPT-2\n",
        "def generate_response(user_input):\n",
        "    # Tokenize and encode the user input\n",
        "    inputs = response_tokenizer.encode(user_input, return_tensors='pt')\n",
        "\n",
        "    # Generate a response from GPT-2\n",
        "    outputs = response_model.generate(inputs, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=1.5)\n",
        "\n",
        "    # Decode and return the generated response\n",
        "    response_text = response_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response_text\n",
        "\n",
        "# Example Usage:\n",
        "user_input = \"Hello, how's the weather today?\"\n",
        "intent = classify_intent(user_input)\n",
        "response = generate_response(user_input)\n",
        "\n",
        "print(f\"Intent: {intent}\")\n",
        "print(f\"Response: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVU5SkaDNaNc",
        "outputId": "ae098e4b-fd7c-4877-c04f-f159957c18bc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n",
            "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent: goodbye\n",
            "Response: Hello, how's the weather today?\n",
            "\n",
            "I'm not sure how much rain I've seen in the last few days, but I'm pretty sure it's been pretty good.\n",
            " (I've been to the beach a few times, and I can tell you that it was pretty nice.)\n",
            ". . .\n",
            ", I think I'll be back in a couple of days. I don't know if I will be able to get back to work, or if it will take me a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzTt5y9sO8x0",
        "outputId": "b5874b4c-abcf-4ae8-e06c-fb801e01d341"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load pre-trained DistilBERT model and tokenizer for intent classification\n",
        "intent_tokenizer = BertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "intent_model = BertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
        "\n",
        "# Dummy labels (you can expand this based on your dataset)\n",
        "intent_labels = [\"greeting\", \"weather\", \"goodbye\"]\n",
        "\n",
        "# Label encoder for encoding the labels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(intent_labels)\n",
        "\n",
        "def classify_intent(user_input):\n",
        "    # Tokenize input\n",
        "    inputs = intent_tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = intent_model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_class_id = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    # Convert prediction to label\n",
        "    intent = label_encoder.inverse_transform([predicted_class_id])[0]\n",
        "    return intent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAt8aO6dQAfw",
        "outputId": "7fb4d68b-bcfd-4fac-dd12-2a6e2a82ceff"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n",
            "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "response_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "response_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "def generate_response(user_input):\n",
        "    # Tokenize user input\n",
        "    inputs = response_tokenizer.encode(user_input, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate a response\n",
        "    outputs = response_model.generate(inputs, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
        "\n",
        "    # Decode the generated response\n",
        "    response = response_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ],
      "metadata": {
        "id": "LQYYo9JxQRmP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmVlGWNOiGnA",
        "outputId": "805dcb24-a478-459d-beb6-37eb9f8c5ad8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.1)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install waitress"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RWxPvcel9BZ",
        "outputId": "ac981fed-dc31-4890-9be1-18131665d489"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: waitress in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok # Reinstall the pyngrok package"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaq77uvs3080",
        "outputId": "c05a385e-f50b-4e0f-fa77-217fb88c7362"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok, conf\n",
        "from waitress import serve\n",
        "from threading import Thread\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "slack_token = 'xoxb-8030636412515-8027900605269-8DHIovyWpFpEGfvLe6w5XbaS'  # Replace with your actual bot token\n",
        "slack_channel = 'ChatBot'  # Replace with the channel you want to post to\n",
        "\n",
        "# Function to send a message to Slack\n",
        "def send_slack_message(message):\n",
        "    url = 'https://slack.com/api/chat.postMessage'\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {slack_token}',  # Your bot token\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "    data = {\n",
        "        'channel': slack_channel,  # The channel you want to post in\n",
        "        'text': message\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    return response.json()  # You can return response to debug if needed\n",
        "\n",
        "# Dummy function for intent classification\n",
        "# Replace this with your actual logic for intent classification\n",
        "def classify_intent(user_input):\n",
        "    # Simple example, classify intent based on keywords\n",
        "    if \"hello\" in user_input.lower():\n",
        "        return \"greeting\"\n",
        "    elif \"bye\" in user_input.lower():\n",
        "        return \"farewell\"\n",
        "    else:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Dummy function for generating a response\n",
        "# Replace this with actual logic to generate a meaningful response\n",
        "def generate_response(intent, user_input):\n",
        "    if intent == \"greeting\":\n",
        "        return \"Hello! How can I assist you today?\"\n",
        "    elif intent == \"farewell\":\n",
        "        return \"Goodbye! Have a great day!\"\n",
        "    else:\n",
        "        return f\"I'm sorry, I don't understand the intent of: {user_input}\"\n",
        "\n",
        "@app.route('/slack', methods=['POST'])\n",
        "def slack_event():\n",
        "    data = request.json\n",
        "    user_input = data.get('text')\n",
        "\n",
        "    # Slack verification challenge\n",
        "    if \"challenge\" in data:\n",
        "        return jsonify({\"challenge\": data[\"challenge\"]})\n",
        "\n",
        "    # Handle messages\n",
        "    if user_input:\n",
        "        # Classify the intent based on the user input\n",
        "        intent = classify_intent(user_input)\n",
        "\n",
        "        # Generate a response based on the intent\n",
        "        response = generate_response(intent, user_input)\n",
        "\n",
        "        return jsonify({\"text\": response})\n",
        "\n",
        "    # Handle case where 'text' is not provided\n",
        "    return jsonify({\"error\": \"No text provided\"}), 400\n",
        "\n",
        "# Function to run Flask in a separate thread\n",
        "def run_flask():\n",
        "    serve(app, host=\"0.0.0.0\", port=5002)\n",
        "\n",
        "# Start Flask in a background thread\n",
        "thread = Thread(target=run_flask)\n",
        "thread.start()\n",
        "\n",
        "# Configure ngrok with your authtoken\n",
        "# Get your authtoken from your ngrok dashboard: https://dashboard.ngrok.com/auth\n",
        "conf.get_default().auth_token = \"2ol4OdYXMiKT2qFwgi0kZX6M2x6_4FDUekNbJQMBujNZtJrLM\"  # Replace YOUR_AUTHTOKEN with your actual token\n",
        "\n",
        "# Disconnect existing tunnels before starting a new one\n",
        "for tunnel in ngrok.get_tunnels():\n",
        "    print(f\"Disconnecting existing tunnel: {tunnel.public_url}\")\n",
        "    ngrok.disconnect(tunnel.public_url)\n",
        "\n",
        "# Start ngrok and print public URL\n",
        "public_url = ngrok.connect(5002)\n",
        "print(\"Public URL:\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBsKoV3vNQZp",
        "outputId": "8f602154-0226-42d6-a6bb-14aaa7b919d3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-20 (run_flask):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-49-be27e87a47a3>\", line 71, in run_flask\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/waitress/__init__.py\", line 13, in serve\n",
            "    server = _server(app, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/waitress/server.py\", line 78, in create_server\n",
            "    last_serv = TcpWSGIServer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/waitress/server.py\", line 243, in __init__\n",
            "    self.bind_server_socket()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/waitress/server.py\", line 364, in bind_server_socket\n",
            "    self.bind(sockaddr)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/waitress/wasyncore.py\", line 379, in bind\n",
            "    return self.socket.bind(addr)\n",
            "OSError: [Errno 98] Address already in use\n",
            "WARNING:pyngrok.process.ngrok:t=2024-11-14T15:56:58+0000 lvl=warn msg=\"Stopping forwarder\" name=http-5002-5be68c85-18a8-42ea-b2ba-51e2e8a05b1c acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disconnecting existing tunnel: https://05d0-35-188-5-58.ngrok-free.app\n",
            "Public URL: NgrokTunnel: \"https://fb57-35-188-5-58.ngrok-free.app\" -> \"http://localhost:5002\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X POST https://fb57-35-188-5-58.ngrok-free.app/slack -H \"content-type: application/json\" -d '{\"text\": \"Hello, world!\"}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7JBDUW888Z9",
        "outputId": "4f666764-d4a2-4fdd-c4be-1f8fad812ad9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"text\":\"Hello! How can I assist you today?\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X POST https://fb57-35-188-5-58.ngrok-free.app/slack -H \"content-type: application/json\" -d '{\"text\": \"Hello, what are you doing\"}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMjfXTqlOFhj",
        "outputId": "204a5e40-0b9e-4d48-8b71-5cf0e126e669"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"text\":\"Hello! How can I assist you today?\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X POST https://fb57-35-188-5-58.ngrok-free.app/slack -H \"content-type: application/json\" -d '{\"text\": \"what is the time?\"}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIeb3gXZOOlf",
        "outputId": "56bc6a7b-0fb3-4877-9068-a74613938d5f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"text\":\"I'm sorry, I don't understand the intent of: what is the time?\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X POST https://fb57-35-188-5-58.ngrok-free.app/slack -H \"content-type: application/json\" -d '{\"text\": \"Bye\"}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36dVOWx9PKqY",
        "outputId": "2cd85c1a-5e34-4c6a-e868-b5ebb8ebf7f8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"text\":\"Goodbye! Have a great day!\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_pcKzPU8Szvm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}